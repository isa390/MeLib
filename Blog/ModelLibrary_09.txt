<br>2022-06-16 12:13:03: 通过代码来配置 URL 拦截规则和请求 URL 所需要的权限，这样就比较死板，如果想要调整访问某一个 URL 所需要的权限，就需要修改代码。  动态管理权限规则就是我们将 URL 拦截规则和访问 URL 所需要的权限都保存在数据库中，这样，在不改变源代码的情况下，只需要修改数据库中的数据，就可以对权限进行调整。<br><br>2022-05-20 16:35:43: 分布式链路追踪  随着互联网业务快速扩展，企业的业务系统变得越来越复杂，不少企业开始向分布式、微服务方向发展，将原本的单体应用拆分成分布式、微服务。这也使得当客户端请求系统的接口时，原本在同一个系统内部的请求逻辑变成了需要在多个微服务之间流转的请求。  单体架构中可以使用AOP在调用具体的业务逻辑前后分别打印一下时间即可计算出整体的调用时间，使用 AOP捕获异常也可知道是哪里的调用导致的异常。  但是在分布式微服务场景下，使用AOP技术是无法追踪到各个微服务的调用情况的，也就无法知道系统中处理一次请求的整体调用链路。  另外，在分布式与微服务场景下，我们需要解决如下问题：  如何快速发现并定位到分布式系统中的问题。 如何尽可能精确的判断故障对系统的影响范围与影响程度。 如何尽可能精确的梳理出服务之间的依赖关系，并判断出服务之间的依赖关系是否合理。 如何尽可能精确的分析整个系统调用链路的性能与瓶颈点。 如何尽可能精确的分析系统的存储瓶颈与容量规划。 如何实时观测系统的整体调用链路情况。 上述问题就是分布式链路追踪技术要解决的问题。所谓的分布式链路追踪，就是将对分布式系统的一次请求转化成一个完整的调用链路。这个完整的调用链路从请求进入分布式系统的入口开始，直到整个请求返回为止。并在请求调用微服务的过程中，记录相应的调用日志，监控系统调用的性能，并且可以按照某种方式显示请求调用的情况。  在分布式链路追踪中，可以统计调用每个微服务的耗时，请求会经过哪些微服务的流转，每个微服务的运行状况等信息。<br><br>2022-05-20 16:29:27: 学习语法糖的意义  互联网时代，有很多标新立异的想法和框架层出不穷，但是，我们对于学习来说应该抓住技术的核心。然而，软件工程是一门协作的艺术，对于工程来说如何提高工程质量，如何提高工程效率也是我们要关注的，既然这些语法糖能辅助我们以更好的方式编写备受欢迎的代码，我们程序员为什么要 抵制 呢？  语法糖也是一种进步，这就和你写作文似的，大白话能把故事讲明白的它就没有语言优美、酣畅淋漓的把故事讲生动的更令人喜欢。<br><br>2022-05-20 16:22:45:漏桶算法  算法思想 与令牌桶是“反向”的算法，当有请求到来时先放到木桶中，worker以固定的速度从木桶中取出请求进行相应。 如果木桶已经满了，直接返回请求频率超限的错误码或者页面  适用场景 流量最均匀的限流方式，一般用于流量“整形”，例如保护数据库的限流。先把对数据库的访问加入到木桶中，worker再以db能够承受的qps从木桶中取出请求，去访问数据库。不太适合电商抢购和微博出现热点事件等场景的限流，一是应对突发流量不是很灵活，二是为每个user_id/ip维护一个队列(木桶)，workder从这些队列中拉取任务，资源的消耗会比较大。<br><br>2022-05-20 16:10:45:区别于监控系统，Kubeprober 从巡检的角度来验证集群的各项功能是否正常，监控作为正向链路，无法覆盖系统中的所有场景，即使系统中各个环境的监控数据都正常，也无法保证系统是 100% 可用的，因此我们就需要一个工具从反向来证明系统的可用性，根本上做到先于用户发现集群中不可用的点，比如：  集群中的所有节点是否均可以被调度，有没有特殊的污点存在等； pod 是否可以正常的创建，销毁，验证从 Kubernetes，Kubelet 到 Docker 的整条链路； 创建一个 service，并测试连通性，验证 kube-proxy 的链路是否正常； 解析一个内部或者外部的域名，验证 CoreDNS 是否正常工作； 访问一个 ingress 域名，验证集群中的 ingress 组件是否正常工作； 创建并删除一个 namespace，验证相关的 webhook 是否正常工作； 对 Etcd 执行 put/get/delete 等操作，用于验证 Etcd 是否正常运行； 通过 mysql-client 的操作来验证 MySQL 是否正常运行； 模拟用户对业务系统进行登录，操作，验证业务的主流程是否正常； 检查各个环境的证书是否过期； 云资源的到期检查；<br><br>2022-05-20 15:59:52:之前我们介绍了一个非常优秀的细粒度控制JAVA线程的库：java thread affinity。使用这个库你可以将线程绑定到特定的CPU或者CPU核上，通过减少线程在CPU之间的切换，从而提升线程执行的效率。<br><br>2022-05-20 15:54:35:基于小样本数据和大数据集之间的这种相关性，研究者提出名为元匹配（meta-matching）的方法。这一方法可以将大数据集上训练出来的机器学习模型迁移到小数据集上，从而训练出更可靠的模型，以更准确地预测新的表型。<br><br>2022-05-20 15:50:01:预估系统主要的逻辑包含三个部分：  特征查询：根据业务从分布式存储中查询用户、场景、item等特征，解析之后准备下一阶段的特征抽取；  特征抽取：根据模型定义的输入特征，对上一阶段查询回来的特征逐个计算，转换成模型需要的数据格式，例如embedding、hash等；  前向推理：将特征抽取后的数据输入机器学习库，经过一系列的矩阵运算等操作，最后输出一个用户对一个item的得分；  三个阶段中，特征查询属于IO密集型，其他两个阶段属于CPU密集型，尤其是前向推理阶段，需要完成大量的矩阵相乘等操作，并且期间有大量的内存申请和读写逻辑。  当前云音乐在线预估系统部署数十线网集群，上线数百物理机。系统上线初期主要使用至强E5 56核机器，去年开始采购的都是至强Gold 96核机器，两种机型具体配置如下：<br><br>2022-05-20 15:25:11: 元数据 是业务、应用、基础设施自身的结构描述，而 时序数据 是他们随时间变化的状态描述。因此，所有表又分为时序表和维度表。维度表用来承载元数据，时序表用来存储被监控对象的时序观测数据，同时两者之间可以进行关联。  最终，所有的数据都组成了一张大网，同时做好了对任何系统的结构描述与状态描述。<br><br>2022-05-20 15:21:44: 在提升整体架构的性能水平上，重点的工作是做了算子下推。这个操作的核心思想都是就近计算，通过调度计算逻辑，减少数据的搬运。算子下推这个能力，经过自动化分析之后，甚至可能直接查询agent的数据；或者中心数据大规模聚合可以做到尽量分级提取，能在单机完成的聚合直接聚合给出部分结果再由中心聚合。<br><br>2022-05-20 15:07:41: 我们可以借助该特性实现一个防抖的案例。  例如我们要实现一个搜索框的功能。文字输入过程中会自动发起搜索请求。为了防止请求发送过于频繁，在高频输入时，不发送接口请求，如果超过了 500ms 下一次输入事件还没有发生，那么就自动请求一次。<br><br>2022-05-20 11:52:24:运行时，协调器通过基于ZooKeeper的leader竞选机制决出leader节点，并由leader节点负责前述任务分配工作。<br><br>2022-05-20 11:49:33:QMQ延迟消息服务架构  延迟消息从生产者投递至延迟服务后，堆积在服务器本地磁盘中。当延迟消息调度时间过期后，延迟服务转发至实时Broker供消费方消费。<br><br>2022-05-20 11:17:24: 究其背后的根源是Spring实现事务通过ThreadLocal把事务和当前线程进行了绑定。ThreadLocal作为本地线程变量载体，保存了当前线程的变量，并确保所有变量是线程安全的。这些封闭隔离的变量中就包含了数据库连接，Session管理的对象以及当前事务运行的其他必要信息，而开启的新线程是获取不到这些变量和对象的。<br><br>2022-05-20 11:16:07: Spring的确可负责事务管理的所有底层实现细节，而且不管你用的是什么持久层框架，如Hibernate、MyBatis，即便是JDBC也都提供了统一的事务模型，确保数据访问方式的变更不会影响到代码实现层面。事务管理的良好封装，一方面提升了开发效率，但同时也要注意到其降低了开发者了解底层原理的动机和意愿。<br><br>2022-05-19 16:16:24:  对于 web 服务来说，为防止非法参数对业务造成影响，在 Controller 层一定要做参数校验的！大部分情况下，请求参数分为如下两种形式：  POST 、 PUT 请求，使用 requestBody 传递参数；  GET 请求，使用 requestParam/PathVariable 传递参数。  下面我们简单介绍下 requestBody 和 requestParam/PathVariable 的参数校验实战！  requestBody 参数校验  POST 、 PUT 请求一般会使用 requestBody 传递参数，这种情况下，后端使用** DTO 对象**进行接收。 只要给 DTO 对象加上 @Validated 注解就能实现自动参数校验 。比如，有一个保存 User 的接口，要求 userName 长度是 2-10 ， account 和 password 字段长度是 6-20 。如果校验失败，会抛出 MethodArgumentNotValidException 异常， Spring 默认会将其转为 400（Bad Request） 请求。  DTO 表示数据传输对象（Data Transfer Object），用于服务器和客户端之间交互传输使用的。在 spring-web 项目中可以表示用于接收请求参数的 Bean 对象。<br><br>2022-05-19 14:10:02:在LevelDB中 SSTable 是整个数据库最重要的结构，所有的SSTable文件本身的内容是 不可修改 的，虽然通常数据在内存中操作，但是数据不可能无限存储，当数据到达一定量之后就需要持久化到磁盘中，而压缩合并的处理就十分考验系统性能了，为此LevelDb使用分层的结构进行存储，下面我们从外部的使用结构开始来了解内部的设计。<br><br>2022-05-19 14:06:22:网易云信的解决方案是在媒体服务器里进行 GOP 缓存，缓存最近 1-2 个 GOP 的媒体包在 Server 端。当客户端和媒体器媒体连接成功后，先发送 GOP 缓存里的媒体包，再发送当前的媒体数据。客户端在收到媒体包后，需要根据一定的策略对齐音视频包，再加速追帧。  在具体的实践过程中，需注意 GOP 缓存大小、客户端的 Jitter buffer 大小的配合、GOP 缓存里音视频的对齐、不同的推流端不同 GOP 长度的适配等情况。  Pacer 平滑发送  如果推流端设置的 Gop 比较大，当拉流客户端媒体连接成功后，一股脑的给客户端发送全部的 Gop 里数据，可能造成客户端缓冲溢出以及其他问题。这时候就需要 Server 的 Pacer 平滑发送发挥作用了。  具体的实践过程中，要注意 Pacer 追帧速率和客户端追帧速率的配合。<br><br>2022-05-19 14:01:14: 在典型直播架构中，左边是推流客户端，协议上才采用 RTMP 上行。右边是拉流客户端，支持不同的拉流协议拉流，比较常见的是：RTMP、FLV,、HLS。  现有架构的优点  这套框架很好的利用了 CDN 厂商或者说云厂商的能力。尽管拉流协议没有统一，但由于 rtmp/flv/hls 等拉流协议是比较成熟的流媒体协议，经过多年的发展，各家 CDN 厂商广泛支持。在云端能力的支持下，服务端并发能力和拉流端加速能力大大增加了，直播行业蓬勃发展。<br><br>2022-05-19 13:58:29:数据采集这块是我们自研的 Qmonitor 系统，它类似于 Prometheus 的 scrape 模块，Prometheus 是一个 all in one 的系统，它将数据采集、存储、查询、异常检测这些全都放在一个系统中，这样做的好处是部署和运维比较简单，但是 Watcher 是将这些组件拆分开的，保证每个组件的独立性和扩展性。用户引用 QmonitorClient 包在应用中埋点，然后暴露出一个 http 接口的 url，Server 端每隔一段时间抓取数据分析聚合数据然后 Push 到真正的存储集群。  一开始遇到的便是采集指标数据的问题，主要的问题集中在以下 3 点：  要采集数据多，目前去哪儿网每分钟需要采集的应用指标在上亿级，同时根据这上亿级指标会聚合出来大概 4 千万左右的汇总指标  海量的聚合计算，上面提到过每一个应用实例中暴露出的每一个指标都需要跟应用的节点数（实例数）做聚合计算，根据上亿的单机指标，会聚合出对应的汇总指标。  为什么需要汇总指标，这里举一个例子，比如你有一个应用，这个应用部署了 100 台主机因此就有 100 个实例在运行，每个实例都会记录自己的接口  foo 的访问次数，比如指标名叫 foo.access.qps，那么通常我们在看 foo.access.qps 的时候是希望以应用为维度的，我们通常更关心当前整个应用的  foo 接口的 qps 而不是某一个实例的。  要求时效性高，必须在 1 分钟内完成所有抓取、计算和推送任务。  因此一开始我们的设计基本需求便是水平扩展的能力要足够好，所以我们采用了 Master-Worker 这样的架构。   如上图所示，Master 节点定时从 DB 中获取所有的任务，然后通过 MQ 分发，Worker 端消费并且处理数据，而 Worker 采用 Python 的多进程多线程模型，将处理后的数据推送到后端存储集群上。这本质上就是一个典型的 Producer/Consumer 模型，这种模型扩展性好而且开发简单，因此我们第一版的数据采集应用便是基于此开发的。  但是随着任务量和指标量的增长，慢慢的出现了几个瓶颈：  任务量增多了之后，从 DB 中获取全量任务，并且通过 MQ 派发时所消耗的时长也越来越长了，时效性不能保证  Python 应用做大量聚合计算时，CPU 消耗比较高，需要更多的机器资源才能满足当前采集任务。  因此我们对这种模型做了优化调整，如下图所示。   总体上仍然是 Master-Worker 这样的一个架构，但是我们取消掉了 MQ 的任务分发，每个 Worker 一旦启动会将自己注册到 etcd，Master 发现有 Worker 变更就会触发 Rebalance 行为，Rebalance 后 Worker 节点便能拿到自己所属的 Task 并将这些 Task 缓存下来，Worker 节点变成了有状态的节点，这样的好处是每分钟不用分派 Task，Task 是一开始就分配好了的，节省了这个分配的时间。Master 和 Worker 通过 etcd 来进行事件通信，Master 监听到 DB 中有任务变更（新增/删除/修改等），会通过 etcd 通知到对应的 worker，Worker 更新自己缓存的 Task。如果其中一个 Worker 挂掉了，那么他负责的 Task 会被 Master 重新分配调度到其他 Worker 上继续执行，而 Master 也通过主从选举的方式保障自己的高可用。   上图是 Worker 内部的逻辑结构图，Worker 大概的一个工作流程是这样的，每个 Worker 内部根据自己的定时器触发任务执行，一旦任务触发 scraper 便会从自己缓存的 Task 中拿到要采集的 url 信息，并发采集，采集回数据后根据不同的客户端类型放入到对应的解析器中，因为去哪儿网早期有多套埋点方式，因此需要兼容不同的客户端埋点格式，解析后的数据会生成统一的 SingleMetric 对象，SingleMetric 对象描述了指标的名称、数据、Tag、所属主机等等信息，生成的SingleMetric 对象会被放在缓存池中，当某个 AppCode 所有的 SingleMetric 采集完毕，聚合器从缓存池中取出这一组 SingleMetric 进行聚合计算，计算后生成 Metric 对象放入发送队列，Sender 从发送队列中取出 Metric Push 到后端存储集群。  新应用我们采用 Go+Gorotouine 的方式开发，对于资源消耗更低，吞吐量更大。整体改造重构后我们用现在的 15 台 worker 机替换了原来的 50+ 的 worker 主机，便能满足目前的数据采集需求，整体任务执行时延也控制在 1 分钟之内。  2.2 存储集群压力大  上面提到过我们后端的存储集群使用的是 Graphite+Whisper 做的二次开发，它自带了数据接收器(cabron-relay)+聚合器(carbon-aggregator)+存储器(carbon-cache)+时序 DB(whisper)这样的一套组件，是基于 Twisted 异步框架开发。而其中原生的 carbon-aggregator 组件在实现上有一些性能问题，在数据量大的时候内存和 CPU 的消耗都是巨大的，还有内存泄漏的风险。<br><br>2022-05-19 13:51:11:一般来说一个基本的监控系统至少需要具备三方面的能力：  数据采集  从各个数据源上抓取数据并存储到时序 DB 中，或者由数据源自己讲数据 Push 到收集器中，然后存储到时序 DB  数据展示  以不同类型的图或组织方式，将收集到的数据展示给用户，并提供通用的聚合能力，用户查看和分析数据  异常告警  对上报的数据做静态阈值或动态阈值之类的异常检测，将异常信息或指标通知给相关用户  再往深做，会有一些偏智能化的探索和实践，比如：  根因推荐  产生告警后根据应用拓扑、日志、事件等信息，综合分析，尝试分析出当前告警的原因，并生成报告推荐给用户，帮助用户快速定位问题。  故障自愈  产生故障或告警后，提取故障或告警特征，匹配对应的应急预案，执行预案，恢复故障。  另外当公司内部告警量过多时就会产生一些告警治理的需求，比如告警抑制、降噪和收敛，甄别出真正有用的告警精准的通知，对于一些频繁报警又没有人处理的进行降噪处理，告警量大时针对某一些业务维度进行聚合收敛等。另外告警需要进行链路追踪，一个告警事件出来后，需要知道都在什么时候通知给了什么人，有没有处理。<br><br>2022-05-19 13:46:12:针对一个 feature 的开发，代码完成并验测验了功能的有效性后，我们需要做一些基础的 checklist 检查，比如测试用例对分支、条件和语句是否都已覆盖，动态分配的内存是否在各种场景下（正常退出和异常退出）都已释放，打开的文件描述符是否都已关闭（另一种可能的资源泄露），在中间过程增加的引用计数是否已减去（这个可能比较隐晦，比如 "kern_path" 和 "path_put" 应配套使用）。<br><br>2022-05-19 13:38:37:分布式事务框架，在这些理论基础上，都进行了或多或少的修订，也有不少创新。比如 LCN 框架（lock，confirm，notify），就抽象出了控制方和发起方的概念，感兴趣的可以自行了解。<br><br>2022-05-19 13:37:26:最大努力补偿  最大努力补偿，是一种衰减式的补偿机制。  拿个最简单的例子来说吧。如果你是微信支付的接入方，微信支付成功之后，它会将支付结果推送到你指定的接口。  微信支付+你的支付结果处理，就可以算是一个大的分布式事务。涉及到微信的系统还有你的自有系统。  如果你的系统一直处理不成功，那么微信支付就会一直不停的重试。这就叫最大努力补偿，用在系统内和系统间都是可以的。  但也不能无限的重试，重试的间隔通常会随着时间衰减。常用的衰减策略有。  messageDelayLevel = 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h 上面的公式，意味着如果一直无法处理成功，将在 1s...，最大 2 小时后重试。如果还不成功，就只能进入人工处理通道。  最大努力补偿只是一种思想，实际的应用有多种方式。比如，我首先将事务落地到消息队列，然后依靠消息队列的重试机制，来达到最大努力补偿的效果，这些都是可行的方案。<br><br>2022-05-19 13:34:02:其实 SAGA 的概念很好理解，你就按照正常的业务逻辑去执行就行了。只不过如果在任何一步发生了异常，就要把前面所提交的数据全部回滚（补偿）。唯一特殊的是，它通常是通过消息驱动来完成事务运转的。  如果你非要追求它的本质，那就是 SAGA 和 TCC 一样，都是先记录执行轨迹，然后通过不断地重试达到最终状态。   上图是 rob vettor 所绘制的一个典型的 SAGA 事务拆分图。在图中，黑色的线为正常业务流程，红色的线为补偿业务流程。  这是一个简单的电子商务结账流程，整个交易跨了 5 个微服务，可以说是非常大的长事务了。  可以看到，这样的事务流转，靠文字描述已经是不好理解了，所以 SAGA 通常会配备一个流程编辑器，直接来把事务编排的过程可视化。<br><br>2022-05-19 13:30:31: 拿资金转账来说，try 就是冻结金额；confirm 就是完成扣减；cancel 就是解冻，只要对应的订单号是一直的，多次执行也不会有任何问题。  由于 TCC 事务的发起方，直接在业务节点即可完成，和 TCC 的代码在同一个地方。  所以，TCC 并不需要一个额外的协调者和事务处理器，它存放在本地表或者资源中即可。  是的，它也要记录一些信息，哪怕是 HashMap 里，否则它根据啥回滚呢？<br><br>2022-05-19 13:27:41: TCC 就是大名鼎鼎的补偿事务，是互联网环境最常用的分布式事务。它的核心思想是：为每一个操作，都准备一个确认动作和相应的补偿动作，一共 3 个方法。  与其靠数据库，不如靠自己的代码！2PC，3PC，都和数据库绑的死死的，TCC 才是码农的最爱（意思就是说，你要多写代码）。   如上图，TCC 同样分为三个阶段，但非常的粗暴： try 尝试阶段：尝试锁定资源  confirm 确认阶段：尝试将锁定的资源进行提交  cancel 取消阶段：其中某个环节执行失败，将发起事务取消动作<br><br>2022-05-19 13:20:14: | 问题  3PC 理论上是比较优秀的，还能够避免阻塞问题，但它多了一次网络通信。如果参与者的数量比较多，网络质量比较差的情况下，这个开销非常可观。它的实现也比较复杂，在实际应用中，是不太多的。<br><br>2022-05-19 13:17:20:Spring 也有 JTA 的事务管理器：  Atomikos、bitronix 实现了 JTA，它们只需要提供 jar 包就可以了。实现了 XA 协议的数据库或者消息队列，已经能够具备了准备、提交、回滚的各种能力<br><br>2022-05-19 12:33:52:备阶段，也叫做 voting 阶段。所谓的 voting，就是参与者告知协调者，自己的资源到底是能够提交（代表它准备好了），还是取消本次事务（比如发生异常）。  这个投票比较有意思，只要有一个参与者返回了 false，本次事务就需要终止，然后执行 rollback。只有全票通过，才会正常 commit。协调者将这个结果，周知所有参与者的这个过程，就是二阶段。<br><br>2022-05-18 11:49:35:大致实现的逻辑就是在每次调用保存日志的同时执行自己的逻辑，比如格式化、通知等。  函数劫持，在一个函数运行之前就把它劫持下来，添加我们想要的功能。当这个函数实际运行的时候，它已经不是原本的函数了，而是被我们添加上去的功能。这也是我们常见的钩子函数的原理之一。  如上面的示例，一般函数劫持会分成三步 :  使用新的变量保存被劫持函数 新函数中改写被劫持函数 新函数中调用原有的函数(保存在变量中的函数) 为什么可以这么做?  一开始，我看上面这段代码还有疑惑，当重新给 saveLog 赋值的时候，不会改变 originSaveLog 的引用指向么?事实上是不会的，只会将 saveLog 指向另外一个引用地址。<br><br>2022-05-18 11:38:49:为了简单起见，我们将在本文中使用 AtomicReference 和 AtomicInteger，但同样的原则适用于其他原子类型。  The set() 方法  在调用set()后，当我们从不同的线程使用get()方法访问该字段时，该变化是立即可见的。这意味着该值被从CPU缓存中刷新到了所有CPU核共有的内存层。<br><br>2022-05-18 10:40:23:由于我们不直接控制电机的角度，这种方法提供了更稳定的操作，由于较小的行动空间而简化了策略训练，并产生了更强大的策略。强化学习策略网络的输入包括先前的步态参数、机器人的高度、基座方向、线性、角速度和反馈，这些信息可以显示机器人是否已经接近设定的安全触发器。对于每个任务，我们都会采用同样的设定。  我们训练一个安全恢复策略，对尽快达到稳定状态给予奖励。此外，在设计安全触发集时，我们也从 可捕捉性理论 中得到了灵感。尤其是，最初的安全触发器集被定义为，确保机器人的脚不会踩在能够利用安全恢复策略进行安全恢复的位置之外。我们使用了一个随机策略，在一个真实的机器人上对安全触发集进行了微调，以防止机器人跌倒。  现实世界的实验结果  我们报告了现实世界的实验结果，显示了奖励学习曲线和高效步态、猫步和两腿平衡任务中安全恢复策略激活的百分比。为了确保机器人能够学会安全，我们在触发安全恢复策略时增加了一个惩罚。这里，所有的策略都是从头开始训练的，除了两腿平衡任务，由于需要更多的训练步骤，所以在模拟中进行了预训练。<br><br>2022-05-18 10:20:16:安装客户端后，一般会在当前硬件设备硬盘上建立数据库，用于存储实时性比较强、数据量比较大的数据。通过破解数据库我们便能拿到必要的数据，比如企业微信的外部联系人列表即存储在本地的sqllite数据库里。<br><br>2022-05-18 10:18:53:业微信账号A，在手机客户端上向客户B（个人微信）发送一条消息，内容“我是周杰伦” 这条消息内容（连同发送者、接收者等信息）通过网络上报到企业微信服务端 企业微信服务端做一系列动作，如判断A和B是否是好友、B是否合法等等之后，将消息发送给客户B 客户B收到消息后，服务商通过API接口将这条消息拉取到自己的数据库里 同时，如果企业微信A在PC客户端处于登陆状态，企业微信服务端会将刚刚发送消息这件事告诉企业微信PC客户端，在PC客户端上也就能看到刚刚发送给客户B的聊天记录<br><br>2022-05-17 18:07:52:防止用户识别  除了前面详述的网络安全方面，私有预取代理还可以防止服务器在预取时通过先前存储在其设备上的信息来识别用户。目前，Chrome 会限制只有用户没有 Cookie 或其他本地状态的网站才能使用私有预取代理方案。以下是通过 Private Prefetch Proxy 发出的预取请求的限制：  (1) Cookies：预取请求不允许携带 Cookies。  如果资源有 Cookie，Chrome 只会发送不带 Cookie 的请求，也不会使用响应内容。 对预取请求的响应中可以包含 Cookie，但只有在用户跳转到预取页面时才会在浏览器保存这些 Cookie。 (2) 指纹识别：其他可用于指纹识别的数据(例如 User-Agent)也进行了调整。Prefetch Proxy 发送的 Header 只携带有限的信息。  Google 也正在计划将 Private Prefetch Proxy 扩展到带有 Cookie 的网站，同时利用一些其他的方案来保障用户隐私。  缓存  即使资源已经在缓存中了，Chrome 也会预取资源，但它们不会携带任何条件请求头，例如 ETag 或 If-Modified-Since(这些 Header 中包含服务器设置的值，即使没有 Cookie 也可用于用户跟踪)。进行这样的预取措施是为了防止将客户端的缓存状态泄漏到预取的网站。此外，如果用户决定跳转到已经预取的网站，Chrome 只会将预取的资源提交到缓存中。  开始使用私有预取代理  对于需要数据预取的网站  对于大部分普通站点，我们希望在其他网站导航到我们网站的时候更快。<br><br>2022-05-17 18:01:36:当用户访问网页时，浏览器从服务器请求 HTML。服务器返回 HTML 响应，然后 HTML 会告诉浏览器下一步的工作，包括请求 CSS、JavaScript、字体和图像等资源。这些资源返回后吗，浏览器还会做一些其他的评估工作，最终在页面上进行布局和渲染。  实际上，大部分时间都花费在了从浏览器到服务器之间的传输上了。根据 Google Chrome 的统计显示，网页大约 40% 的可见延迟都花费在浏览器等待服务器返回的第一个字节上了。  数据预取  那么， 如果可以预取网页上所需的资源文件，也就是在用户访问这些页面之前就获取它们，这将给网页带来巨大的性能提升。  数据预取后，网页在可以正常显示之前只剩下了评估、布局和渲染工作了。   数据预取  实际上，我们一些常见的性能优化的手段：  rel="dns-prefetch"：向浏览器声明在接下来的页面中即将用到某个域名下的资源，要求浏览器尽可能早的提前发起对该域名的 dns 解析操作。 rel="preconnet"：告诉浏览器页面即将使用某域名下的资源，可以让浏览器提前建立网络连接，在页面真正发起资源请求时，会使用已经建立的网络连接，直接跳过这些耗时建连操作。 这些都属于数据预取的措施，我们可以做到预取一些我们当前站点的主要资源。  在同站的情况下，这些手段很容易实施，但是对于跨站的常见，就不那么简单了。<br><br>2022-05-17 17:46:53:在微服务中，不同的服务可能是不同的团队来进行开发，服务端接口的修改、滚动发布等都需要有很好的兼容性和可用性。<br><br>2022-05-17 17:46:05:在微服务架构中，会将一个完整的应用程序拆分成一组服务。这些服务之间需要经过协作，通过接口调用，才能组成一个完整的应用。  不同的服务部署在不同的机器上，或者同一个机器的多个容器中，进程间进行通信就不可避免了，也变得非常重要。<br><br>2022-05-17 17:39:05:CRLF是”回车 + 换行”（\r\n）的简称。在HTTP协议中，HTTP Header与HTTP Body是用两个CRLF分隔的，浏览器就是根据这两个CRLF来取出HTTP 内容并显示出来。所以，一旦我们能够控制HTTP 消息头中的字符，注入一些恶意的换行，这样我们就能注入一些会话Cookie或者HTML代码，所以CRLF Injection又叫HTTP Response Splitting，简称HRS<br><br>2022-05-17 17:25:03:二/三方库对接方面，如图3所示，我们设计了协议层对接各个可插拔的组件，将 SDK 的一些基础能力如网络、数据库、文件、埋点、图片加载等进行接口抽象，使得业务接入时可以方便的代替为自定义的基础能力。<br><br>2022-05-17 17:02:11:我们把不同时间收到的，描述一个或多种特征随时间发生变化的数据称为时间序列数据，例如商品的每天的销量，股价每天的涨跌等等。对于这类数据，我们往往会希望通过算法去预测其未来的走势，这类算法统称为时序预测算法。  时序预测算法在电商场景中有着大量的应用场景，例如我们需要预测商品未来的销量，来决定补多少货；需要预测APP未来的流量，来决定具体的搜推策略等等。  基于上述需求背景，我们开发沉淀了一套时序预测算法系统，实现了“ 数据-模型-服务 ”的完整链路，并落地于网易严选的补货、调拨、投放、搜索、推荐等多个业务场景。<br><br>2022-05-17 16:58:00:Sentinel(哨兵)是 Redis 高可用(high availability) 解决方案，由一个或者多个 Sentinel 实例(instance)组成的 Sentinel 系统(system)可以监视一个或者多个 Redis 主服务器和其跟随的从服务器，并且在被监视的主服务进入下线状态时，自动进行将当前主服务器的从服务器其中一个升级为主服务器，然后将下线的主服务器设置为新主服务器的从节点。<br><br>2022-05-17 16:52:54:首先我们来解决下什么是推模式，顾名思义，推模式就是我推给你。在MQ中也就是Broker收到消息后主动推送给Consumer的操作，叫做推模式。  推模式的实现是客户端会与服务端（Broker）建立长连接，当有消息时服务端会通过长连接通道将消息推送给客户端，这样客户端就能实时消费到最新的消息。<br><br>2022-05-17 16:52:26:拉模式肯定不能用传统的定时拉取，定时长及时性无法保证，定时短，在没有消息的情况下对服务端会一直请求。所以很多拉模式都是基于长轮询来实现。  长轮询就是客户端向服务端发起请求，如果此时有数据就直接返回，如果没有数据就保持连接，等到有数据时就直接返回。如果一直没有数据，超时后客户端再次发起请求，保持连接，这就是长轮询的实现原理。很多的开源框架都是用的这种方式，比如配置中心Apollo的推送。<br><br>2022-05-17 16:48:38:Zookeeper提供了一个Watch机制，可以让客户端感知到Zookeeper Server上存储的数据变化，这样一种机制可以让Zookeeper实现很多的场景，比如配置中心、注册中心等。   Watch机制采用了Push的方式来实现，也就是说客户端和Zookeeper Server会建立一个长连接，一旦监听的指定节点发生了变化，就会通过这个长连接把变化的事件推送给客户端。  Watch的具体流程分为几个部分：  首先，是客户端通过指定命令比如 exists 、 get ，对特定路径增加watch  然后服务端收到请求以后，用HashMap保存这个客户端会话以及对应关注的节点路径，同时客户端也会使用HashMap  存储指定节点和事件回调函数的对应关系。  当服务端指定被watch的节点发生变化后，就会找到这个节点对应的会话，把变化的事件和节点信息发给这个客户端。  客户端收到请求以后，从ZkWatcherManager里面对应的回调方法进行调用，完成事件变更的通知。<br><br>2022-05-17 16:45:34:经技术专家最终鉴定后发现，韩冰使用了 "shred "和 "rm "命令来擦除数据库，rm 命令删除了文件的符号链接，shred 则用多种模式将数据覆盖了三次，导致数据不可恢复。<br><br>2022-05-17 16:37:59:STUN（Session Traversal Utilities for NAT）是一个工具协议，这个协议的主要目的是协调帮助两个在 NAT 之后的设备建立 UDP （也可以是 TCP）传输；既然 STUN 是一个协议，那我们就可以采用任意技术栈来开发实现一个 STUN 服务及 STUN 客户端，实现的 STUN 主要有两个作用：  帮助获取客户端的公网地址，并通过复杂的策略，探测出客户端所处的 NAT 类型；  STUN 还可以帮助两个客户端之间进行 NAT “打洞”或者协调 TURN 在两个客户端中间充当中继服务；<br><br>2022-05-17 16:21:43:两个数据库分别部署在两台服务器上，相互同步数据，但是只有一个提供给外部访问，当一个宕机后，另外一个可以继续提供服务，在没有 keepalived 软件的帮助下，只能手动切换  keepalived 监测、自动重启、流量切换  检测和重启：两台服务器上都部署 keepalived 软件，定时检测 MySQL 服务是否正常，如果一个数据库服务崩了，keepalived 会用脚本尝试重启 mysql 服务。  备份：两个 keepalived 服务都提供了虚拟 IP 供客户端使用，但是流量只会转到一台 MySQL 服务上。  虚拟 IP：keepalived 配置好了后，会有一个 虚拟 IP，对于客户端来说，不关心连接的是哪台 MySQL，访问虚拟 IP 就可以了。  流量切换：如果客户端正在访问的 MySQL 服务崩了后，keepalived 会用我们写的脚本自动重启 MySQL，如果重启失败，脚本主动停掉 keepalived，客户端的流量就不会访问到这台服务器上的 MySQL 服务，后续访问的流量都会切到另外一台 MySQL 服务。  检测和重启的原理如下所示：   需要配置的内容如下：  两台 Ubuntu 服务器上启动 MySQL 容器。  配置 MySQL 主从复制架构。  将 MySQL 主从改为主主复制架构。  两台服务器搭建 keepalived 环境监控 MySQL 和自动重启 MySQL。  二、主主复制的原理  对于 MySQL 的主主架构，其实原理就是两台服务器互为主从，双向复制。而复制的原理如下：  主从复制主要有以下流程：  主库将数据的改变记录到 binlog 中；  从库会在一定时间间隔内对master 的 binlog 进行检查，如果发生改变，则开始一个 I/O Thread 请求读取 master 中 binlog ；  同时主库为每个 I/O 线程启动一个 dump 线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从库将启动 SQL 线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后 I/O Thread 和 SQL Thread 将进入睡眠状态，等待下一次被唤醒；<br><br>2022-05-17 15:16:09:但是通过学习房屋特征（地板材质、墙面材质、面积大小、房屋年龄、房屋建造结构）以及地理属性（是否临街、所在区域、是否存在地铁、周围配套情况）以及历史二手房交易价格，得到的二手房交易价格预测模型，就是一个很典型的机器学习应用场景。因为尽管我们知道房屋特征、地理属性这些多维度的变量与二手房交易价格之间存在规律特征，但这种规律特征足够复杂，以至于有必要借助机器学习对于规律特征进行探索。<br><br>2022-05-17 15:05:18:2022年5月12日18:45 CST 更新 Amprius Technologies Inc.管理人士说，该公司已与一家特殊目的收购公司(SPAC)达成协议，协议对这家硅阳极电池制造商的估值约为13亿美元，并推动其上市。 总部位于美国加州弗里蒙特的Amprius称其生产的电池比传统的锂离子电池功率更大，因为这种电池的阳极使用了高能量密度的硅，而不是传统的材料石墨。该公司目前将此电池出售给空中客车(Airbus SE, EADSY)和美国陆军部(U.S. Army)等客户，用于电动飞机和无人机，并表示该电池也可用于电动汽车。<br><br>2022-05-17 14:13:14:如果对Java有所了解的同学应该听说过log4j，在log4j2中引入了一个名为Disruptor的组件，它让日志处理飞快了起来，受到很多Java开发者的追捧。Disruptor之所以这么厉害，是因为它使用了无锁并发、环形队列、缓存行填充等多种高级技术。  相比之下，Golang的Channel虽然也使用了环形缓冲，但是还是使用了锁，作为队列来说性能并不是最优的。<br><br>2022-05-17 14:09:49:工程实践中，我们往往还需要对日志进行采集，将日志归集到一起，然后用于各种处理分析，比如生产环境上的错误分析、异常告警等等。在日志消息系统领域，Kafka久负盛名，这篇文章就以将日志发送到Kafka来实现日志的采集；同时考虑到日志分析时对结构化数据的需求，这篇文章还会提供一种输出Json格式日志的方法。<br><br>2022-05-17 14:02:28:AJAX 是“Asynchronous JavaScript and XML”的缩写，尽管严格地说，开发人员并不需要使用异步方法、JavaScript 或 XML。我们现在将通用的“Ajax”术语表示任何从服务器获取数据、更新 DOM 而无需刷新整个页面的客户端过程。<br><br>2022-05-17 14:01:04:Ajax 是大多数 web 应用程序背后的核心技术，它允许页面向 web 服务发出异步请求，因此数据可以不经过页面往返服务器无刷新显示数据。  术语 Ajax 不是一种技术，相反，它指的是从客户端脚本加载服务器数据的方法。<br><br>2022-05-17 12:57:26:考虑一个自动驾驶车辆，其目标是从初始状态x0达到期望的目标状态xg，同时考虑其自身动力学（1）和周围环境施加的约束。规划的目标是在不违反约束的情况下找到最优轨迹。为此，NMPC问题可表述为：   在（3）中把目标状态作为参考。  运动规划的任务是使自动驾驶车在各种条件或情况下，其性能与人类相当甚至更好，关键是有能力的模型。尽管有这种实用性，基于物理的车辆模型仍有实际局限性，可能存在误差或偏差，或者只适合某些特定条件。与此同时，自动驾驶车获取的数据越来越丰富，构建精确、广泛适用的数据驱动模型成为可能。其中，神经网络在基于模型的车辆控制方面显示出了巨大的优势。因此，利用神经网络模型解决运动规划问题。此外，NMPC基于采样的实现比数值优化更适合神经网络的控制。最近的研究还表明，在处理大规模问题时，采样比数值优化更具竞争力。  采用前馈神经网络表示车辆动力学：  1） 车辆动力学参数化：用神经网络对车辆动力学进行参数化有不同的方法。一种简单的方法是将状态变量和控制变量反馈给神经网络，并使网络预测系统下一个状态。然而，由于采样时间短，学习这样的函数可能很困难、 这导致状态递推中非常相似，导致网络有效地学习一个identity transformation 。因此，重点放在状态转换函数的参数化，因为它显示了状态的增量变化。参数化由下式给出：   2） 数据收集和预处理：从考虑的车辆模型中生成和收集训练数据。这个实现是从均匀分布中采样一组状态，也从均匀分布中采样一个随机控制变量。这里状态变量和控制变量分别由不等式约束（3c）定义。然后，将这些应用于车型，并记录系统的下一个状态。这些构造了训练所需的转换变量。神经网络的输入是状态和控制对，相应的输出是增量状态变化。每个元素的输入和输出几乎都是归一化的，确保每个元素的输入和输出对控制的贡献也是相等的。获得的训练数据集包含100万个数据点。       神经网络结构和训练：用一个密集的神经网络，四个隐层，大小分别为200，300，100，ReLU为激活函数。然后，通过最小化均方误差来训练神经网络   规划车辆或自车具有一个状态xk，包括其2D位置（xpk，ykp）、线速度νk和航向角ψk，作为单轨车辆模型。其控制输入uk包括加速度ak和转向角δk。仿真研究中考虑三个约束：道路边界、障碍物避撞和有限的控制动作。    <br><br>2022-05-17 12:51:35:1.实现了集群中节点的相互发现，可以开始Gossip了 2.指定了它们之间的主从关系 <br><br>2022-05-17 12:48:25:window.requestAnimationFrame() 告诉浏览器你希望执行一个动画，并且要求浏览器在下次重绘之前调用指定的回调函数更新动画。  这里借用 MDN 的描述，顾名思义就是传入一个函数，让浏览器在下一次渲染之前进行调用。<br><br>2022-05-17 12:37:56:管道式编程  Unix 操作系统有一个管道机制，可以把前一个操作的值传给后一个操作。这个机制非常有用，使得简单的操作可以组合成为复杂的操作。<br><br>2022-05-17 12:10:21:通过重定向车主的 手机 或key fob遥控钥匙与汽车之间的通信，外人可以欺骗特斯拉进入系统，使其认为车主就在汽车附近。  卡西姆·汗称，虽然他是在一款特斯拉车型上演示了这项技术，但是该入侵技术并不是专门针对特斯拉的。相反，这是他对特斯拉的无钥匙进入系统进行修补的结果，该系统依赖于所谓的低能耗蓝牙(BLE)协议。没有证据表明窃贼利用这一入侵技术不正当地进入特斯拉汽车。特斯拉尚未置评。<br><br>2022-05-17 11:50:42:接口幂等性的处理，同一个接口，在短时间内接收到相同参数的请求，接口可能会拒绝处理。那么在判断的时候，就需要先把请求的参数提取出来进行判断，如果是 JSON 参数，此时就会有问题，参数提前取出来了，将来在接口中再去获取 JSON 参数，就会发现没有了.        这里我们可以利用装饰者模式对 HttpServletRequest 的功能进行增强，具体做法也很简单，我们重新定义一个 HttpServletRequest：         <br><br>2022-05-16 18:33:33:最后一步显然，将上述优化好的中间格式转换成我们真正需要的汇编，由汇编器翻译成机器码，大功告成。  通过这篇文章，可以了解LLVM的基本概念，LLVM的精髓就在于，你不必对上面每一个步骤内部如何实现的彻底了解细节。你只需要基于LLVM就能很快设计出属于自己的编译器。<br><br>2022-05-16 18:32:51:这一步就要分配寄存器了。在Register Allocation之前我们认为寄存器其实是可以无限用的，但实际硬件的寄存器有限的。所以我们得考虑寄存器数量与寄存器值的生命周期，将虚拟的寄存器替换成实际的寄存器。这个一般会用到图着色等等算法，贼复杂，好在LLVM都实现好了，不用在重复造轮子。<br><br>2022-05-16 18:31:01:这个主要负责将你的LLVM IR转换为有向无环图，便于后续利用图算法优化。  例如将下面的LLVM IR 转换成图，每个节点是一个指令。<br><br>2022-05-16 18:30:23:经过词法分析、语法分析、语义分析、LLVM IR生产，最终将C++转化成后端认可的LLVM IR。  词法分析：将编程语言取出一个个词，遇到不认识的字符就报错。  例如将a=b+c 拆成a,= ,b ,+, c  语法分析：将语法提取出来，例如你写了个a+b=c, 明显不符合语法，直接报错  语义分析：分析一下你写的代码实际含义是不是对，例如a=b+c, a,b,c有没有定义，类型是不是对的  LLVM IR生产：经过上述三步，将代码转化成树状描述（抽象语法树），然后再转化成IR定义的IR即可。<br><br>2022-05-16 17:59:25:我们将继续代表客户进行创新。我们将继续朝着端到端的现代化数据战略使命迈进。因为正如我之前提到的，“没有数据库是孤岛”。  客户不再只想在他们的数据库中存储和查询数据。然后，他们想要分析这些数据以创造价值，无论是更好的个性化或推荐引擎，还是可以使用机器学习运行预测分析的预测系统。  将点对点连接起来，并继续使 Amazon DynamoDB 更安全、更可用、更高性能和更易于使用，这将是我们永无止境的旅程。<br><br>2022-05-16 17:57:55:Amazon DynamoDB 背后的想法来自于与 SmugMug 和 Flickr 的首席执行官 Don MacAskill 等客户的讨论。其实，他们就是最早带有互联网属性的公司，而在当时像这样的互联网公司也在快速走向市场。在线用户数量呈爆炸式增长，数据模式不固定，追求快速交付，轻运维，是他们的特征。将所有数据存储在一个盒子中的传统关系数据库模型无法很好地扩展，它迫使用户重新分片他们的关系数据库，然后管理所有的分区和重新分区等等。  这对我们来说并不新鲜。这些挑战是我们构建原始 Amazon Dynamo 的原因，当时它还不是一项服务，还是一个由亚马逊工程师操作的软件系统。在我们的一次客户咨询会议上，Don 说：“你们都启动了 Amazon Dynamo，并展示了可扩展的非关系数据库系统的可能性。为什么我们不能将其作为外部服务？”  当时，所有高级亚马逊云科技高管都在场，这也是我们当时问自己的一个问题。Don 不是唯一需要它的客户，越来越多的客户想要那种无需处理分区和重新分区的可扩展数据库，而且他们还想要极高的可用性。逐渐的，我们开始认真思考构建一个不受 SQL 限制的可扩展云数据库。  Amazon DynamoDB 与原始 Amazon Dynamo 不同，因为它实际上是通过几个原始 Amazon Dynamo 组件搭建了一易于使用的云服务。 我们的客户不再需要配置集群，通过创建一个表（Table）存储数据，就可以实现并无缝缩放；他们不必任何操作，甚至无需安装单个库来操作数据库。  Amazon Dynamo 到 Amazon DynamoDB 的这种演变非常重要， 因为我们真正以前所未有的方式拥抱云 ，以及它的弹性和可扩展性。  我们于 2012 年 1 月 18 日推出它，一经推出就大受欢迎。Don MacAskill 的公司和其他几家公司开始使用它。从发布之日起，不仅弹性，而且个位数的延迟性能都与客户产生了非常好的共鸣。我们进行了相当多的创新，从协议层一直到 SSD 存储的底层存储层，以及我们启用的各种其他功能。最早的生产项目之一是有一个有趣用例的客户；他们在做超级碗（Super Bowl：全国橄榄球联盟决赛，全美直播的体育界春晚）广告。  因为 Amazon DynamoDB 非常有弹性，它可以无缝地扩展到每秒 100,000 次写入 ，然后在超级碗结束后缩减，这样他们就不会再产生额外成本。  这在技术领域也是个大事件，现在我们司空见惯的横向扩展与弹性，对当时的数据库来说还不具备这样的想象空间。但是 Amazon DynamoDB 为云而构建的架构使所有这些横向扩展用例成为可能 ，这也是 Amazon DynamoDB 现在支持多个高流量 Amazon 站点和系统（包括 Alexa、Amazon.com 和所有 Amazon）的履行中心的原因之一。  去年，在我们 66 小时的 Prime Day 期间，这些来源进行了数万亿次 API 调用，而 Amazon DynamoDB 以个位数毫秒的性能保持了高可用性，达到每秒 8920 万个请求的峰值。  自 2012 年以来，我们增加了许多创新，不仅是因为它的基本可用性、耐用性、安全性和规模，而且还因为它的易用性功能。  我们不止步于键值存储，还支持基于哈希的分区和基于范围的分区，并且我们增加了对二级索引的支持，以支持更复杂的查询功能——同时不影响规模或可用性。   我们现在还支持通过适用于 Amazon DynamoDB 的 Amazon Kinesis Data Streams 捕获可扩展的流式数据。我坚信任何数据库的工作都不应该是一个孤岛，更不可能是死胡同。它应该生成变化的数据流，然后使用它来将其连接到分析应用程序或其他数据存储。  我们继续在备份和恢复等功能上进行全面创新。对于像 Amazon DynamoDB 这样具有数百万个分区的大型数据库系统，进行备份和恢复并非易事，许多伟大的创新都致力于让客户轻松体验这种体验。  我们还添加了 创建全局表（Global Table） 的功能，以便客户可以实现数据库负载的全球覆盖的同时，具备本地读写性能。然后我们增加了使用 Amazon DynamoDB 进行事务的能力，所有这些都着眼于您如何继续保持 Amazon DynamoDB 围绕可用性和可扩展性的使命。  最近，我们还致力于更强的成本优化。客户通常需要长期存储数据，虽然这些旧数据可能很少被访问，但它必须保持高度可用。  例如，社交媒体应用程序的最终用户很少访问较旧的帖子和上传的图像，但应用程序必须确保在请求时可以立即访问这些工件。这种不经常访问的数据可能会给客户带来巨大的存储费用，因为它们的数量不断增长，并且存储这些数据的成本相对较高，因此客户在这些情况下通过编写代码将较旧、访问频率较低的数据从 Amazon DynamoDB 移动到成本较低的存储来优化成本 Amazon S3 等替代品。  因此，在最近的 re:Invent 大会上，我们推出了 Amazon DynamoDB Standard-Infrequent Access 表类 ，这是一个新的经济高效的表类，用于存储不经常访问的数据，同时保持 Amazon DynamoDB 的高可用性和性能。  我们将保持 Amazon DynamoDB 的最初愿景作为指引，持续创新，以帮助客户提供更易于查询的用例、进行复杂的全局事务复制的能力，同时继续管理成本。<br><br>2022-05-16 17:06:17:如果我们不熟悉区块链是如何工作的，这里有一个快速的回顾。  区块链由数据区块组成。  区块存储在分布式节点上。  区块链中的每个节点都充当一个“迷你服务器”，它允许操作者读写数据区块。  添加到区块链的任何区块都必须在网络上的所有节点之间传播，以实现同步。  在考虑Web3框架中的数据访问层时，链上存储是标准，因为它本质上是不可变的，并允许任何公众个人查看/验证它。  与区块浏览器一样，数据提供者是区块链交互层的关键部分。对于初学者来说，它们提供了一个进入较低网络层的窗口，并作为一个在线资源，用于检索有关交易、地址余额、gas 费用等的实时和历史数据。区块浏览器通常用于查找支付的关键细节、确定不同交易的状态或简单地了解区块链使用的总体趋势。数据提供者为公众提供了阅读和解释区块链的能力。<br><br>2022-05-16 16:54:07:Kubernetes 中提供的存活探测器来探测什么时候进⾏容器重启。例如，存活探测器可以捕捉到死锁（应⽤程序在运⾏，但是⽆法继续执⾏后⾯的步骤）。在这样的情况下重启容器有助于让应⽤程序在有问题的情况下更可⽤。<br><br>2022-05-16 16:33:08:https://3d-moments.github.io/<br><br>2022-05-16 16:27:03:这就是谷歌、康奈尔大学、华盛顿大学最近联合推出的成果，能只用 2张相近 的照片还原出3D瞬间，目前已被CVPR 2022收录。  论文作者一作、二作均为华人，一作小姐姐本科毕业于浙江大学。  用2张照片正反向预测中间场景  这种方法适用于两张非常相似的照片，比如连拍时产生的一系列照片。  方法的关键在于将2张图片转换为一对基于特征的 分层深度图像 （LDI），并通过场景流进行增强。  整个过程可以把两张照片分别看做是“起点”和“终点”，然后在这二者之间逐步预测出每一刻的变化。  具体来看，过程如下：   首先，将两张照片用单应矩阵（homegraphy）对齐，分别预测两张照片的稠密深度图。  然后将每个RGBD图像转换为彩色的LDI，通过深度感知修复背景中被遮挡的部分。  其中，RGB图像即为普通RGB图像+深度图像。   之后用二维特征提取器修复LDI的每个颜色层，以获取特征层，从而生成两份特征图层。  下一步就到了 模拟场景运动 部分。  通过预测两个输入图像之间的深度和光流，就能计算出LDI中每个像素的场景流。  而如果想要两张图之间渲染出一个新的视图、并提升到3D，在此需要将两组带特征值的LDI提升到一对3D点云中，还要沿着场景流双向移动到中间的时间点。  然后再将三维的特征点投影展开，形成正向、反向的二维特征图及对应深度图。  最后将这些映射与时间线中对应时间点的权重线性混合，将结果传给图像合成网络，就能得到最后的效果了。  实验结果  从数据方面来看，该方法在所有误差指标上，均高于基线水平。   在UCSD数据集上，这一方法可以保留画面中的更多细节，如（d）所示。   在NVIDIA数据集上进行消融实验表明，该方法在提高渲染质量上表现也很nice。   不过也存在一些问题：当两张图像之间的改变比较大时，会出现物体错位的现象。  比如下图中酒瓶的瓶嘴移动了，不该发生变化的酒杯也摇晃了起来。   还有照片如果没有拍全的地方，在合成的时候难免会出现“截肢”的情况，比如下图中喂考拉的手。<br>